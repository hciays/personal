{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b_WassLXm_V"
   },
   "source": [
    "# **INSTRUCTIONS FOR SUBMISSION**\n",
    "### For the coding questions, please enter the relevant code in the code cell provided below the respective questions. For the numerical questions, write your answer in the text cell provided below the respective questions (you can also embed your latex-macros for formula, symbols, equations, etc.), or scan your paper-written solution and embed it in the text cell below respective question. After adding your answers (code or numericals), save the python notebook as \"A5\\_yourname.ipynb\" and submit the same on STUDIP.\n",
    "\n",
    "**Note: Only ipynb will be accepted as assignment submissions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-Vv0ZnqQpBC"
   },
   "source": [
    "# **Part A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kMf-qGyNzKl"
   },
   "source": [
    "### **Q1.** The hidden Markov model for POS tagging with bigram assumption is $\\hat{T}=\\underset{T}{\\operatorname{argmax}}\\underset{i}{\\operatorname{\\Pi}} p(w_i|t_i)\\times p(t_i|t_{i-1})$. \n",
    "\n",
    "Answer the following questions and justify your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvTwVmW_N0rq"
   },
   "source": [
    "#### **1(a)** Write similar characteristic equation for $\\hat{T}$ with trigram assumption for tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YJowS2cN7Lh"
   },
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE602R2pN9vV"
   },
   "source": [
    "#### **1(b)** Write similar characteristic equation for $\\hat{T}$ with unigram assumption for tags if all tags appear with same frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOhc1hnMOaSi"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfeyb8TDOeRs"
   },
   "source": [
    "### **Q2** Consider a language $X$ having only $3$ part-of-speech tags: $\\alpha$, $\\beta$ and $\\gamma$. $X$ follows a bigram dependency in terms of POS tags, i.e., the tag of a word depends only on the tag of its previous word. There are $n$ unique words in $X$'s dictionary: $x_1,x_2,\\cdots,x_n$. The planet where $X$ is used has a central server having a huge corpus with POS tags. Answer the following questions with justifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1U71Q9HOjwO"
   },
   "source": [
    "#### **2(a)** A batch of university students are asked to design a POS tagger for $X$ as an assignment. Given a sentence, the POS tagger has to find all tags accurately to pass the case, and the criteria for students to succeed is to design a POS tagger with at least $70$% accuracy over a set of randomly selected sentences. While all students start coding for hidden Markov model, a lazy student $S$ tries to find a shortcut. $S$ finds that all sentences are of length $3$, always starting with a word tagged $\\beta$, and each tag appears once in every sentence. $\\gamma$ comes after $\\alpha$ $70$% of the times. So, $S$ writes a module which always prints the same series of tags irrespective of the input sentence, and eventually succeeds in the automatic assignment evaluation. What is that tag series always printed by $S$'s POS tagger? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZd97DtIOnog"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VezOCCmbOrOS"
   },
   "source": [
    "#### **2(b)** Unfortunately, the tutor catches the cheat code used by $S$ to pass in the assignment evaluation. So, the totor decides to take a test to check whether $S$ has understood the concept or not. The tutor reveals some additional patterns observed in the corpus: a $\\beta$ tagged word is $x_k$ with probability $P(x_k|\\beta)=\\frac{P(x_{k-1}|\\beta)}{2}$ for $1<k\\leq n$. A $\\alpha$ tagged word is $x_k$ with probability $P(x_k|\\alpha)=\\frac{P(x_{k-1}|\\alpha)}{3}$ for $1<k\\leq n$. A $\\gamma$ tagged word is $x_k$ with probability $P(x_k|\\gamma)=P(x_{k-1}|\\gamma)$ for $1<k\\leq n$. Find the ratio of probabilities of a sentence \"$x_1 x_2 x_3$\" being POS-tagged as \"$\\beta\\gamma\\alpha$\" vs \"$\\beta\\alpha\\gamma$\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCPQ79Z7OrnU"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iDhm3TyQ3ca"
   },
   "source": [
    "# **Part B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsNCAQLIQ430"
   },
   "source": [
    "### **Q1** Consider a language $X$ with only three POS tags: $\\alpha$, $\\beta$, $\\gamma$. \n",
    "\n",
    "The transition probabilities are as given in the following table.\n",
    "\n",
    "|  | $\\alpha$  |$\\beta$ |$\\gamma$ | end| \n",
    "| ----------- | ----------- | ----------- |----------- | ----------- |\n",
    "| start | 0.5| 0.3| 0.2| -|\n",
    "| $\\alpha$ | 0| 0.5| 0.3|0.2 |\n",
    "| $\\beta$ | 0.2|0.1 |0.4 | 0.3|\n",
    "| $\\gamma$ | 0.3| 0.1| 0.1|0.5 |\n",
    "\n",
    "Following the hidden Markov model for POS tagging, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0gJuBQ0RARD"
   },
   "source": [
    "#### **1(a)** Out of all sentences of length $5$, calculate the probability of a random sentence being tagged $\\alpha\\gamma\\beta\\gamma\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-eKt-KjREID"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGc4qO7eRPsT"
   },
   "source": [
    "#### **1(b)** If the word-likelihoods (i.e. $P(w|t)$) are as given below, then find the best POS tags for the sentence \"$w_1 w_2 w_3$\" using Viterbi algorithm's optimal path approach.\n",
    "\n",
    "|  | $w_1$  |$w_2$ |$w_3$ |\n",
    "| ----------- | ----------- | ----------- |----------- | \n",
    "| $\\alpha$ | 0.2| 0.1| 0.2|\n",
    "| $\\beta$ | 0.1|0.2 |0.3 |\n",
    "| $\\gamma$ | 0.1| 0.2| 0.2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1BJsyDTRQjc"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSb3KKD2RUQE"
   },
   "source": [
    "### **Q2** Consider a language $X$ with only three POS tags: $A$, $B$, $C$. You are given a corpus without tags. So, you are running a forward-backward algorithm.\n",
    "\n",
    "In some epoch during the process, the parameters ($\\theta$) are encountered as given in the following tables.\n",
    "\n",
    "The initial state distribution and state transition probabilities are as given below.\n",
    "\n",
    "|  | $A$  |$B$ |$C$ | end| \n",
    "| ----------- | ----------- | ----------- |----------- | ----------- |\n",
    "| start | 0.5| 0.3| 0.2| -|\n",
    "| $A$ | 0| 0.5| 0.3|0.2 |\n",
    "| $B$ | 0.2|0.1 |0.4 | 0.3|\n",
    "| $C$ | 0.3| 0.1| 0.1|0.5 |\n",
    "\n",
    "The emission probabilities are as given below.\n",
    "\n",
    "|  | $w_1$  |$w_2$ |$w_3$ |\n",
    "| ----------- | ----------- | ----------- |----------- | \n",
    "| $A$ | 0.2| 0.1| 0.2|\n",
    "| $B$ | 0.1|0.2 |0.3 |\n",
    "| $C$ | 0.1| 0.2| 0.2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQFt_bDIRYNM"
   },
   "source": [
    "#### **2(a)** Calculate the $\\alpha(\\cdot)$s in forward procedure for the sentence $w_1w_2w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZMz_HmQRbVd"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qwCFbGXRf3d"
   },
   "source": [
    "#### **2(b)** Calculate the $\\beta(\\cdot)$s in backward procedure for the sentence $w_1w_2w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkH8nyIzRgoU"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r12Z24wSRjF9"
   },
   "source": [
    "#### **2(c)** Calculate $\\zeta_{AB}(2)$ using the $\\alpha(\\cdot)$ and $\\beta(\\cdot)$ values. Note that $t=1$ at the first word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NXtVgkARja9"
   },
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_Assignment5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
